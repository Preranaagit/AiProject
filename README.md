# AiProject
This project presents a novel approach to skin cancer classification by combining DINO, a self-supervised Vision Transformer (ViT), with text-prompt-based learning. Using image embeddings from DINO and semantic prompts as text representations, we train a model that aligns both modalities to predict skin lesion types. This strategy reduces dependence on annotated data, promotes model interpretability, and is particularly suitable for resource-limited healthcare settings.

At the core, we use DINO ViT-B/16, a vision transformer pretrained without labels, to extract rich features from dermoscopic images. Text prompts like “a photo of melanoma” are encoded using SentenceTransformer (MiniLM) models. These image and text embeddings are then compared using cosine similarity. The model is trained using either contrastive loss or triplet loss, allowing it to learn semantic alignment between visual features and class descriptions.We use the HAM10000 dataset, a public medical imaging dataset containing over 10,000 dermoscopic images of pigmented skin lesions. Each image is labeled with one of seven classes, including melanoma, basal cell carcinoma, and benign nevi. Accompanying metadata such as patient age, sex, lesion location, and diagnostic method helps support future multimodal extensions of this work.

Model Architecture
The model begins by extracting image features using DINO with a ViT-B/16 backbone. These features are matched against embedded class-specific text prompts using a sentence transformer model. Training uses either contrastive or triplet loss to align correct image-prompt pairs. In one variation, a simple MLP classifier is also trained on DINO features for supervised classification. This combination allows the model to be used both as a classifier and as a prompt-based search engine for medical images.
## Link to Dataset : https://www.kaggle.com/datasets/kmader/skin-cancer-mnist-ham10000/data
